{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 4070 Ti (UUID: GPU-9f5777bd-76c3-8ae0-d0e4-17b8a642c8ce)\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install TTS\n",
    "# ! pip install trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gdown\n",
    "# gdown.download(url='https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2')\n",
    "# ! tar -xvjf LJSpeech-1.1.tar.bz2\n",
    "# ! rm LJSpeech-1.1.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Audio\n",
    "from termcolor import colored\n",
    "\n",
    "from typing import Dict, List, Union\n",
    "from torch.cuda.amp.autocast_mode import autocast\n",
    "from trainer.trainer_utils import get_optimizer, get_scheduler\n",
    "\n",
    "from TTS.tts.layers.tacotron.capacitron_layers import CapacitronVAE\n",
    "from TTS.tts.layers.tacotron.gst_layers import GST\n",
    "from TTS.tts.layers.tacotron.tacotron2 import Decoder, Encoder, Postnet\n",
    "from TTS.tts.models.base_tacotron import BaseTacotron\n",
    "from TTS.tts.utils.measures import alignment_diagonal_score\n",
    "from TTS.tts.utils.speakers import SpeakerManager\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "from TTS.tts.utils.visual import plot_alignment, plot_spectrogram\n",
    "from TTS.utils.capacitron_optimizer import CapacitronOptimizer\n",
    "\n",
    "from TTS.tts.configs.tacotron2_config import Tacotron2Config\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "\n",
    "from trainer import Trainer, TrainerArgs\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: \u001b[32mcuda:0\u001b[0m\n",
      "Torch type: \u001b[32mtorch.float16\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "print(\"Device type:\", colored(DEVICE, \"green\"))\n",
    "print(\"Torch type:\", colored(TORCH_DTYPE, \"green\"))\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "DATASET_DIR = 'Datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = BaseDatasetConfig(\n",
    "    formatter=\"ruslan\", \n",
    "    meta_file_train=\"metadata_RUSLAN_22200.csv\", \n",
    "    path=\"Datasets/RUSLAN\",\n",
    "    language=\"ru\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Tacotron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tacotron2(BaseTacotron):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: \"Tacotron2Config\",\n",
    "        ap: \"AudioProcessor\" = None,\n",
    "        tokenizer: \"TTSTokenizer\" = None,\n",
    "        speaker_manager: SpeakerManager = None,\n",
    "    ):\n",
    "        super().__init__(config, ap, tokenizer, speaker_manager)\n",
    "\n",
    "        self.decoder_output_dim = config.out_channels\n",
    "\n",
    "        # pass all config fields to `self`\n",
    "        # for fewer code change\n",
    "        for key in config:\n",
    "            setattr(self, key, config[key])\n",
    "\n",
    "        # init multi-speaker layers\n",
    "        if self.use_speaker_embedding or self.use_d_vector_file:\n",
    "            self.init_multispeaker(config)\n",
    "            self.decoder_in_features += self.embedded_speaker_dim  # add speaker embedding dim\n",
    "\n",
    "        if self.use_gst:\n",
    "            self.decoder_in_features += self.gst.gst_embedding_dim\n",
    "\n",
    "        if self.use_capacitron_vae:\n",
    "            self.decoder_in_features += self.capacitron_vae.capacitron_VAE_embedding_dim\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(self.num_chars, 512, padding_idx=0)\n",
    "\n",
    "        # base model layers\n",
    "        self.encoder = Encoder(self.encoder_in_features)\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            self.decoder_in_features,\n",
    "            self.decoder_output_dim,\n",
    "            self.r,\n",
    "            self.attention_type,\n",
    "            self.attention_win,\n",
    "            self.attention_norm,\n",
    "            self.prenet_type,\n",
    "            self.prenet_dropout,\n",
    "            self.use_forward_attn,\n",
    "            self.transition_agent,\n",
    "            self.forward_attn_mask,\n",
    "            self.location_attn,\n",
    "            self.attention_heads,\n",
    "            self.separate_stopnet,\n",
    "            self.max_decoder_steps,\n",
    "        )\n",
    "        self.postnet = Postnet(self.out_channels)\n",
    "\n",
    "        # setup prenet dropout\n",
    "        self.decoder.prenet.dropout_at_inference = self.prenet_dropout_at_inference\n",
    "\n",
    "        # global style token layers\n",
    "        if self.gst and self.use_gst:\n",
    "            self.gst_layer = GST(\n",
    "                num_mel=self.decoder_output_dim,\n",
    "                num_heads=self.gst.gst_num_heads,\n",
    "                num_style_tokens=self.gst.gst_num_style_tokens,\n",
    "                gst_embedding_dim=self.gst.gst_embedding_dim,\n",
    "            )\n",
    "\n",
    "        # Capacitron VAE Layers\n",
    "        if self.capacitron_vae and self.use_capacitron_vae:\n",
    "            self.capacitron_vae_layer = CapacitronVAE(\n",
    "                num_mel=self.decoder_output_dim,\n",
    "                encoder_output_dim=self.encoder_in_features,\n",
    "                capacitron_VAE_embedding_dim=self.capacitron_vae.capacitron_VAE_embedding_dim,\n",
    "                speaker_embedding_dim=self.embedded_speaker_dim\n",
    "                if self.capacitron_vae.capacitron_use_speaker_embedding\n",
    "                else None,\n",
    "                text_summary_embedding_dim=self.capacitron_vae.capacitron_text_summary_embedding_dim\n",
    "                if self.capacitron_vae.capacitron_use_text_summary_embeddings\n",
    "                else None,\n",
    "            )\n",
    "\n",
    "        # backward pass decoder\n",
    "        if self.bidirectional_decoder:\n",
    "            self._init_backward_decoder()\n",
    "        # setup DDC\n",
    "        if self.double_decoder_consistency:\n",
    "            self.coarse_decoder = Decoder(\n",
    "                self.decoder_in_features,\n",
    "                self.decoder_output_dim,\n",
    "                self.ddc_r,\n",
    "                self.attention_type,\n",
    "                self.attention_win,\n",
    "                self.attention_norm,\n",
    "                self.prenet_type,\n",
    "                self.prenet_dropout,\n",
    "                self.use_forward_attn,\n",
    "                self.transition_agent,\n",
    "                self.forward_attn_mask,\n",
    "                self.location_attn,\n",
    "                self.attention_heads,\n",
    "                self.separate_stopnet,\n",
    "                self.max_decoder_steps,\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def shape_outputs(mel_outputs, mel_outputs_postnet, alignments):\n",
    "        \"\"\"Final reshape of the model output tensors.\"\"\"\n",
    "        mel_outputs = mel_outputs.transpose(1, 2)\n",
    "        mel_outputs_postnet = mel_outputs_postnet.transpose(1, 2)\n",
    "        return mel_outputs, mel_outputs_postnet, alignments\n",
    "\n",
    "    def forward(  # pylint: disable=dangerous-default-value\n",
    "        self, text, text_lengths, mel_specs=None, mel_lengths=None, aux_input={\"speaker_ids\": None, \"d_vectors\": None}\n",
    "    ):\n",
    "        \"\"\"Forward pass for training with Teacher Forcing.\n",
    "\n",
    "        Shapes:\n",
    "            text: :math:`[B, T_in]`\n",
    "            text_lengths: :math:`[B]`\n",
    "            mel_specs: :math:`[B, T_out, C]`\n",
    "            mel_lengths: :math:`[B]`\n",
    "            aux_input: 'speaker_ids': :math:`[B, 1]` and  'd_vectors': :math:`[B, C]`\n",
    "        \"\"\"\n",
    "        aux_input = self._format_aux_input(aux_input)\n",
    "        outputs = {\"alignments_backward\": None, \"decoder_outputs_backward\": None}\n",
    "        # compute mask for padding\n",
    "        # B x T_in_max (boolean)\n",
    "        input_mask, output_mask = self.compute_masks(text_lengths, mel_lengths)\n",
    "        # B x D_embed x T_in_max\n",
    "        embedded_inputs = self.embedding(text).transpose(1, 2)\n",
    "        # B x T_in_max x D_en\n",
    "        encoder_outputs = self.encoder(embedded_inputs, text_lengths)\n",
    "        if self.gst and self.use_gst:\n",
    "            # B x gst_dim\n",
    "            encoder_outputs = self.compute_gst(encoder_outputs, mel_specs)\n",
    "\n",
    "        if self.use_speaker_embedding or self.use_d_vector_file:\n",
    "            if not self.use_d_vector_file:\n",
    "                # B x 1 x speaker_embed_dim\n",
    "                embedded_speakers = self.speaker_embedding(aux_input[\"speaker_ids\"])[:, None]\n",
    "            else:\n",
    "                # B x 1 x speaker_embed_dim\n",
    "                embedded_speakers = torch.unsqueeze(aux_input[\"d_vectors\"], 1)\n",
    "            encoder_outputs = self._concat_speaker_embedding(encoder_outputs, embedded_speakers)\n",
    "\n",
    "        # capacitron\n",
    "        if self.capacitron_vae and self.use_capacitron_vae:\n",
    "            # B x capacitron_VAE_embedding_dim\n",
    "            encoder_outputs, *capacitron_vae_outputs = self.compute_capacitron_VAE_embedding(\n",
    "                encoder_outputs,\n",
    "                reference_mel_info=[mel_specs, mel_lengths],\n",
    "                text_info=[embedded_inputs.transpose(1, 2), text_lengths]\n",
    "                if self.capacitron_vae.capacitron_use_text_summary_embeddings\n",
    "                else None,\n",
    "                speaker_embedding=embedded_speakers if self.capacitron_vae.capacitron_use_speaker_embedding else None,\n",
    "            )\n",
    "        else:\n",
    "            capacitron_vae_outputs = None\n",
    "\n",
    "        encoder_outputs = encoder_outputs * input_mask.unsqueeze(2).expand_as(encoder_outputs)\n",
    "\n",
    "        # B x mel_dim x T_out -- B x T_out//r x T_in -- B x T_out//r\n",
    "        decoder_outputs, alignments, stop_tokens = self.decoder(encoder_outputs, mel_specs, input_mask)\n",
    "        # sequence masking\n",
    "        if mel_lengths is not None:\n",
    "            decoder_outputs = decoder_outputs * output_mask.unsqueeze(1).expand_as(decoder_outputs)\n",
    "        # B x mel_dim x T_out\n",
    "        postnet_outputs = self.postnet(decoder_outputs)\n",
    "        postnet_outputs = decoder_outputs + postnet_outputs\n",
    "        # sequence masking\n",
    "        if output_mask is not None:\n",
    "            postnet_outputs = postnet_outputs * output_mask.unsqueeze(1).expand_as(postnet_outputs)\n",
    "        # B x T_out x mel_dim -- B x T_out x mel_dim -- B x T_out//r x T_in\n",
    "        decoder_outputs, postnet_outputs, alignments = self.shape_outputs(decoder_outputs, postnet_outputs, alignments)\n",
    "        if self.bidirectional_decoder:\n",
    "            decoder_outputs_backward, alignments_backward = self._backward_pass(mel_specs, encoder_outputs, input_mask)\n",
    "            outputs[\"alignments_backward\"] = alignments_backward\n",
    "            outputs[\"decoder_outputs_backward\"] = decoder_outputs_backward\n",
    "        if self.double_decoder_consistency:\n",
    "            decoder_outputs_backward, alignments_backward = self._coarse_decoder_pass(\n",
    "                mel_specs, encoder_outputs, alignments, input_mask\n",
    "            )\n",
    "            outputs[\"alignments_backward\"] = alignments_backward\n",
    "            outputs[\"decoder_outputs_backward\"] = decoder_outputs_backward\n",
    "        outputs.update(\n",
    "            {\n",
    "                \"model_outputs\": postnet_outputs,\n",
    "                \"decoder_outputs\": decoder_outputs,\n",
    "                \"alignments\": alignments,\n",
    "                \"stop_tokens\": stop_tokens,\n",
    "                \"capacitron_vae_outputs\": capacitron_vae_outputs,\n",
    "            }\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, text, aux_input=None):\n",
    "        \"\"\"Forward pass for inference with no Teacher-Forcing.\n",
    "\n",
    "        Shapes:\n",
    "           text: :math:`[B, T_in]`\n",
    "           text_lengths: :math:`[B]`\n",
    "        \"\"\"\n",
    "        aux_input = self._format_aux_input(aux_input)\n",
    "        embedded_inputs = self.embedding(text).transpose(1, 2)\n",
    "        encoder_outputs = self.encoder.inference(embedded_inputs)\n",
    "\n",
    "        if self.gst and self.use_gst:\n",
    "            # B x gst_dim\n",
    "            encoder_outputs = self.compute_gst(encoder_outputs, aux_input[\"style_mel\"], aux_input[\"d_vectors\"])\n",
    "\n",
    "        if self.capacitron_vae and self.use_capacitron_vae:\n",
    "            if aux_input[\"style_text\"] is not None:\n",
    "                style_text_embedding = self.embedding(aux_input[\"style_text\"])\n",
    "                style_text_length = torch.tensor([style_text_embedding.size(1)], dtype=torch.int64).to(\n",
    "                    encoder_outputs.device\n",
    "                )  # pylint: disable=not-callable\n",
    "            reference_mel_length = (\n",
    "                torch.tensor([aux_input[\"style_mel\"].size(1)], dtype=torch.int64).to(encoder_outputs.device)\n",
    "                if aux_input[\"style_mel\"] is not None\n",
    "                else None\n",
    "            )  # pylint: disable=not-callable\n",
    "            # B x capacitron_VAE_embedding_dim\n",
    "            encoder_outputs, *_ = self.compute_capacitron_VAE_embedding(\n",
    "                encoder_outputs,\n",
    "                reference_mel_info=[aux_input[\"style_mel\"], reference_mel_length]\n",
    "                if aux_input[\"style_mel\"] is not None\n",
    "                else None,\n",
    "                text_info=[style_text_embedding, style_text_length] if aux_input[\"style_text\"] is not None else None,\n",
    "                speaker_embedding=aux_input[\"d_vectors\"]\n",
    "                if self.capacitron_vae.capacitron_use_speaker_embedding\n",
    "                else None,\n",
    "            )\n",
    "\n",
    "        if self.num_speakers > 1:\n",
    "            if not self.use_d_vector_file:\n",
    "                embedded_speakers = self.speaker_embedding(aux_input[\"speaker_ids\"])[None]\n",
    "                # reshape embedded_speakers\n",
    "                if embedded_speakers.ndim == 1:\n",
    "                    embedded_speakers = embedded_speakers[None, None, :]\n",
    "                elif embedded_speakers.ndim == 2:\n",
    "                    embedded_speakers = embedded_speakers[None, :]\n",
    "            else:\n",
    "                embedded_speakers = aux_input[\"d_vectors\"]\n",
    "\n",
    "            encoder_outputs = self._concat_speaker_embedding(encoder_outputs, embedded_speakers)\n",
    "\n",
    "        decoder_outputs, alignments, stop_tokens = self.decoder.inference(encoder_outputs)\n",
    "        postnet_outputs = self.postnet(decoder_outputs)\n",
    "        postnet_outputs = decoder_outputs + postnet_outputs\n",
    "        decoder_outputs, postnet_outputs, alignments = self.shape_outputs(decoder_outputs, postnet_outputs, alignments)\n",
    "        outputs = {\n",
    "            \"model_outputs\": postnet_outputs,\n",
    "            \"decoder_outputs\": decoder_outputs,\n",
    "            \"alignments\": alignments,\n",
    "            \"stop_tokens\": stop_tokens,\n",
    "        }\n",
    "        return outputs\n",
    "\n",
    "    def before_backward_pass(self, loss_dict, optimizer) -> None:\n",
    "        # Extracting custom training specific operations for capacitron\n",
    "        # from the trainer\n",
    "        if self.use_capacitron_vae:\n",
    "            loss_dict[\"capacitron_vae_beta_loss\"].backward()\n",
    "            optimizer.first_step()\n",
    "\n",
    "    def train_step(self, batch: Dict, criterion: torch.nn.Module):\n",
    "        \"\"\"A single training step. Forward pass and loss computation.\n",
    "\n",
    "        Args:\n",
    "            batch ([Dict]): A dictionary of input tensors.\n",
    "            criterion ([type]): Callable criterion to compute model loss.\n",
    "        \"\"\"\n",
    "        text_input = batch[\"text_input\"]\n",
    "        text_lengths = batch[\"text_lengths\"]\n",
    "        mel_input = batch[\"mel_input\"]\n",
    "        mel_lengths = batch[\"mel_lengths\"]\n",
    "        stop_targets = batch[\"stop_targets\"]\n",
    "        stop_target_lengths = batch[\"stop_target_lengths\"]\n",
    "        speaker_ids = batch[\"speaker_ids\"]\n",
    "        d_vectors = batch[\"d_vectors\"]\n",
    "\n",
    "        aux_input = {\"speaker_ids\": speaker_ids, \"d_vectors\": d_vectors}\n",
    "        outputs = self.forward(text_input, text_lengths, mel_input, mel_lengths, aux_input)\n",
    "\n",
    "        # set the [alignment] lengths wrt reduction factor for guided attention\n",
    "        if mel_lengths.max() % self.decoder.r != 0:\n",
    "            alignment_lengths = (\n",
    "                mel_lengths + (self.decoder.r - (mel_lengths.max() % self.decoder.r))\n",
    "            ) // self.decoder.r\n",
    "        else:\n",
    "            alignment_lengths = mel_lengths // self.decoder.r\n",
    "\n",
    "        # compute loss\n",
    "        with autocast(enabled=False):  # use float32 for the criterion\n",
    "            loss_dict = criterion(\n",
    "                outputs[\"model_outputs\"].float(),\n",
    "                outputs[\"decoder_outputs\"].float(),\n",
    "                mel_input.float(),\n",
    "                None,\n",
    "                outputs[\"stop_tokens\"].float(),\n",
    "                stop_targets.float(),\n",
    "                stop_target_lengths,\n",
    "                outputs[\"capacitron_vae_outputs\"] if self.capacitron_vae else None,\n",
    "                mel_lengths,\n",
    "                None if outputs[\"decoder_outputs_backward\"] is None else outputs[\"decoder_outputs_backward\"].float(),\n",
    "                outputs[\"alignments\"].float(),\n",
    "                alignment_lengths,\n",
    "                None if outputs[\"alignments_backward\"] is None else outputs[\"alignments_backward\"].float(),\n",
    "                text_lengths,\n",
    "            )\n",
    "\n",
    "        # compute alignment error (the lower the better )\n",
    "        align_error = 1 - alignment_diagonal_score(outputs[\"alignments\"])\n",
    "        loss_dict[\"align_error\"] = align_error\n",
    "        return outputs, loss_dict\n",
    "\n",
    "    def get_optimizer(self) -> List:\n",
    "        if self.use_capacitron_vae:\n",
    "            return CapacitronOptimizer(self.config, self.named_parameters())\n",
    "        return get_optimizer(self.config.optimizer, self.config.optimizer_params, self.config.lr, self)\n",
    "\n",
    "    def get_scheduler(self, optimizer: object):\n",
    "        opt = optimizer.primary_optimizer if self.use_capacitron_vae else optimizer\n",
    "        return get_scheduler(self.config.lr_scheduler, self.config.lr_scheduler_params, opt)\n",
    "\n",
    "    def before_gradient_clipping(self):\n",
    "        if self.use_capacitron_vae:\n",
    "            # Capacitron model specific gradient clipping\n",
    "            model_params_to_clip = []\n",
    "            for name, param in self.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    if name != \"capacitron_vae_layer.beta\":\n",
    "                        model_params_to_clip.append(param)\n",
    "            torch.nn.utils.clip_grad_norm_(model_params_to_clip, self.capacitron_vae.capacitron_grad_clip)\n",
    "\n",
    "    def _create_logs(self, batch, outputs, ap):\n",
    "        \"\"\"Create dashboard log information.\"\"\"\n",
    "        postnet_outputs = outputs[\"model_outputs\"]\n",
    "        alignments = outputs[\"alignments\"]\n",
    "        alignments_backward = outputs[\"alignments_backward\"]\n",
    "        mel_input = batch[\"mel_input\"]\n",
    "\n",
    "        pred_spec = postnet_outputs[0].data.cpu().numpy()\n",
    "        gt_spec = mel_input[0].data.cpu().numpy()\n",
    "        align_img = alignments[0].data.cpu().numpy()\n",
    "\n",
    "        figures = {\n",
    "            \"prediction\": plot_spectrogram(pred_spec, ap, output_fig=False),\n",
    "            \"ground_truth\": plot_spectrogram(gt_spec, ap, output_fig=False),\n",
    "            \"alignment\": plot_alignment(align_img, output_fig=False),\n",
    "        }\n",
    "\n",
    "        if self.bidirectional_decoder or self.double_decoder_consistency:\n",
    "            figures[\"alignment_backward\"] = plot_alignment(alignments_backward[0].data.cpu().numpy(), output_fig=False)\n",
    "\n",
    "        # Sample audio\n",
    "        audio = ap.inv_melspectrogram(pred_spec.T)\n",
    "        return figures, {\"audio\": audio}\n",
    "\n",
    "    def train_log(\n",
    "        self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int\n",
    "    ) -> None:  # pylint: disable=no-self-use\n",
    "        \"\"\"Log training progress.\"\"\"\n",
    "        figures, audios = self._create_logs(batch, outputs, self.ap)\n",
    "        logger.train_figures(steps, figures)\n",
    "        logger.train_audios(steps, audios, self.ap.sample_rate)\n",
    "\n",
    "    def eval_step(self, batch: dict, criterion: nn.Module):\n",
    "        return self.train_step(batch, criterion)\n",
    "\n",
    "    def eval_log(self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int) -> None:\n",
    "        figures, audios = self._create_logs(batch, outputs, self.ap)\n",
    "        logger.eval_figures(steps, figures)\n",
    "        logger.eval_audios(steps, audios, self.ap.sample_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_from_config(config: \"Tacotron2Config\", samples: Union[List[List], List[Dict]] = None):\n",
    "        \"\"\"Initiate model from config\n",
    "\n",
    "        Args:\n",
    "            config (Tacotron2Config): Model config.\n",
    "            samples (Union[List[List], List[Dict]]): Training samples to parse speaker ids for training.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "        from TTS.utils.audio import AudioProcessor\n",
    "\n",
    "        ap = AudioProcessor.init_from_config(config)\n",
    "        tokenizer, new_config = TTSTokenizer.init_from_config(config)\n",
    "        speaker_manager = SpeakerManager.init_from_config(new_config, samples)\n",
    "        return Tacotron2(new_config, ap, tokenizer, speaker_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "espeak\n",
      " | > Found 22200 files in /home/kitsu/course_work/TTS task/Datasets/RUSLAN\n",
      "Tacotron2 params: \u001b[31m28_307_058\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "config = Tacotron2Config(\n",
    "    optimizer=\"RAdam\",\n",
    "    text_cleaner=\"phoneme_cleaners\",\n",
    "    phonemizer='espeak',\n",
    "    use_phonemes=True,\n",
    "    phoneme_language=\"ru\",\n",
    "\n",
    "    test_sentences=['Привет, это Руслан.', 'Рад видеть тебя, мой дорогой слушатель.',\n",
    "                    'Это всего лишь самый обычный тест.', \n",
    "                    'Как насчёт того, чтобы услышать во мне человеческий голос?'],\n",
    "    datasets=[dataset_config],\n",
    "    phoneme_cache_path=os.path.join(\"/home/kitsu/course_work/TTS task\", \"phoneme_cache\")\n",
    ")\n",
    "\n",
    "print(config.phonemizer)\n",
    "\n",
    "ap = AudioProcessor.init_from_config(config, verbose=False)\n",
    "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
    "\n",
    "\n",
    "model = Tacotron2(config, ap, tokenizer, speaker_manager=None)\n",
    "\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    dataset_config,\n",
    "    eval_split=True,\n",
    "    eval_split_max_size=config.eval_split_max_size,\n",
    "    eval_split_size=config.eval_split_size,\n",
    ")\n",
    "\n",
    "str_param_nums = f\"{sum(p.numel() for p in model.parameters()):_}\"\n",
    "print(\"Tacotron2 params:\", colored(str_param_nums, 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: False\n",
      " | > Precision: float32\n",
      " | > Current device: 0\n",
      " | > Num. of GPUs: 1\n",
      " | > Num. of CPUs: 20\n",
      " | > Num. of Torch Threads: 10\n",
      " | > Torch seed: 54321\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      " > Start Tensorboard: tensorboard --logdir=output/run-May-10-2024_12+28AM-0000000\n",
      "\n",
      " > Model has 28307058 parameters\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/1000\u001b[0m\n",
      " --> output/run-May-10-2024_12+28AM-0000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Pre-computing phonemes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/21978 [00:00<06:22, 57.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ˈɑ rˈɑnnʲijɪ? rˈɑnnʲijɪ tˈoʒy ɭʲu\"bɭʲˈu\", zdˈɑɭsʲʌ ˈja.\n",
      " [!] Character '\"' not found in the vocabulary. Discarding it.\n",
      "mˈɑk ˈotʃʲin sʲirɪ^jjˈeznʌ ɡʌtˈovʲiɭsʲʌ kˈɑ pʌbʲˈeɡu. kupʲˈiɭ prʲiʑirvatʲˈivy. napˈoɭnʲiɭ ˈix ʃʌkʌɭˈɑdʌm. vʑˈɑɭ ɡrʲˈeɭku s pʲitʲjjivˈoj vʌdˈoj.\n",
      " [!] Character '^' not found in the vocabulary. Discarding it.\n",
      "ɭʌsʲˈik, sʲˈidʲʌ nə kˈortʌtʃʲkʌx, pʲirʲiɭʲˈistyvʌɭ juɡʌsɭˈɑfskʲij ʒurnˈɑɭ. tˈɑk, skazˈɑɭa vˈɑrʲʌ, pʌjdˈu vzɡɭʲanˈu…\n",
      " [!] Character '…' not found in the vocabulary. Discarding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/21978 [00:00<07:17, 50.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "«rʌʒdˈɑjitsʌ tʃʲiɭʌvʲˈek rʌʒdˈɑjitsʌ tsˈɛɭyj mʲˈir!» nʲe znˈɑju, kʲˈem tˈy stˈɑnʲiʃ. ɭʲimʲbʲˈit?! tˈokʌrʲim ˈiɭʲɪ ʃaxtʲˈerʌm.\n",
      " [!] Character '«' not found in the vocabulary. Discarding it.\n",
      "«rʌʒdˈɑjitsʌ tʃʲiɭʌvʲˈek rʌʒdˈɑjitsʌ tsˈɛɭyj mʲˈir!» nʲe znˈɑju, kʲˈem tˈy stˈɑnʲiʃ. ɭʲimʲbʲˈit?! tˈokʌrʲim ˈiɭʲɪ ʃaxtʲˈerʌm.\n",
      " [!] Character '»' not found in the vocabulary. Discarding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1005/21978 [00:17<06:51, 51.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ˈo knʲˈiɡʌxʃydʲˈevrʌx dvadtsˈɑtʌvʌ vʲˈeka «…tˈut ˈi dˈoktʌr ʒyvˈɑɡʌ“ pʌsʲtʲirnˈɑka.\n",
      " [!] Character '“' not found in the vocabulary. Discarding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1782/21978 [00:31<05:02, 66.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mʲitʃʲtˈɑjim pʌɭutʃʲˈitʲ ot vˈɑs rʲitsˈɛnʑiju.”\n",
      " [!] Character '”' not found in the vocabulary. Discarding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21978/21978 [06:22<00:00, 57.45it/s]\n",
      "\n",
      "\u001b[1m > TRAINING (2024-05-10 00:35:17) \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "> DataLoader initialization\n",
      "| > Tokenizer:\n",
      "\t| > add_blank: False\n",
      "\t| > use_eos_bos: False\n",
      "\t| > use_phonemes: True\n",
      "\t| > phonemizer:\n",
      "\t\t| > phoneme language: ru\n",
      "\t\t| > phoneme backend: espeak\n",
      "\t| > 7 not found characters:\n",
      "\t| > \"\n",
      "\t| > ^\n",
      "\t| > …\n",
      "\t| > «\n",
      "\t| > »\n",
      "\t| > “\n",
      "\t| > ”\n",
      "| > Number of instances : 21978\n",
      " | > Preprocessing samples\n",
      " | > Max text length: 724\n",
      " | > Min text length: 11\n",
      " | > Avg text length: 81.76936026936026\n",
      " | \n",
      " | > Max audio length: 2236400.0\n",
      " | > Min audio length: 26722.0\n",
      " | > Avg audio length: 225577.94826644828\n",
      " | > Num. instances discarded samples: 0\n",
      " | > Batch group size: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403388920/work/aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:35:19 -- STEP: 0/687 -- GLOBAL_STEP: 0\u001b[0m\n",
      "     | > decoder_loss: 5.566230773925781  (5.566230773925781)\n",
      "     | > postnet_loss: 7.580275058746338  (7.580275058746338)\n",
      "     | > stopnet_loss: 0.7268451452255249  (0.7268451452255249)\n",
      "     | > ga_loss: 0.02094024233520031  (0.02094024233520031)\n",
      "     | > decoder_diff_spec_loss: 0.11765585839748383  (0.11765585839748383)\n",
      "     | > postnet_diff_spec_loss: 4.181035995483398  (4.181035995483398)\n",
      "     | > decoder_ssim_loss: 0.9114792943000793  (0.9114792943000793)\n",
      "     | > postnet_ssim_loss: 0.8976041674613953  (0.8976041674613953)\n",
      "     | > loss: 5.645116806030273  (5.645116806030273)\n",
      "     | > align_error: 0.941591739654541  (0.941591739654541)\n",
      "     | > grad_norm: tensor(2.5635, device='cuda:0')  (tensor(2.5635, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 1.3143  (1.314330816268921)\n",
      "     | > loader_time: 0.4073  (0.4073367118835449)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: audio amplitude out of range, auto clipped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:35:32 -- STEP: 25/687 -- GLOBAL_STEP: 25\u001b[0m\n",
      "     | > decoder_loss: 5.911991119384766  (5.84970853805542)\n",
      "     | > postnet_loss: 7.913775444030762  (7.856447830200195)\n",
      "     | > stopnet_loss: 0.7336831092834473  (0.7286118340492248)\n",
      "     | > ga_loss: 0.011039420031011105  (0.013986780047416688)\n",
      "     | > decoder_diff_spec_loss: 0.11938965320587158  (0.11850607484579086)\n",
      "     | > postnet_diff_spec_loss: 4.194687366485596  (4.171238384246826)\n",
      "     | > decoder_ssim_loss: 0.9623643755912781  (0.9616406798362732)\n",
      "     | > postnet_ssim_loss: 0.9465394616127014  (0.9465006351470947)\n",
      "     | > loss: 5.801066875457764  (5.774556274414063)\n",
      "     | > align_error: 0.9744175374507904  (0.9629091743379832)\n",
      "     | > grad_norm: tensor(2.1503, device='cuda:0')  (tensor(2.2031, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 0.3433  (0.2865207004547119)\n",
      "     | > loader_time: 0.2452  (0.22925262451171874)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:35:48 -- STEP: 50/687 -- GLOBAL_STEP: 50\u001b[0m\n",
      "     | > decoder_loss: 5.859645366668701  (5.892880115509033)\n",
      "     | > postnet_loss: 7.853857517242432  (7.897269687652588)\n",
      "     | > stopnet_loss: 0.7289419770240784  (0.7300287210941314)\n",
      "     | > ga_loss: 0.008502144366502762  (0.012012357562780381)\n",
      "     | > decoder_diff_spec_loss: 0.11715660989284515  (0.11818096742033958)\n",
      "     | > postnet_diff_spec_loss: 4.1525115966796875  (4.166538858413695)\n",
      "     | > decoder_ssim_loss: 0.9633039236068726  (0.9624166750907898)\n",
      "     | > postnet_ssim_loss: 0.9475355744361877  (0.9471746528148651)\n",
      "     | > loss: 5.744955539703369  (5.786205768585205)\n",
      "     | > align_error: 0.975520171225071  (0.9677644502744078)\n",
      "     | > grad_norm: tensor(1.9854, device='cuda:0')  (tensor(2.1243, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 0.3603  (0.3135689878463745)\n",
      "     | > loader_time: 0.2533  (0.25082861900329595)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:36:05 -- STEP: 75/687 -- GLOBAL_STEP: 75\u001b[0m\n",
      "     | > decoder_loss: 5.925611972808838  (5.907146898905436)\n",
      "     | > postnet_loss: 7.929833889007568  (7.911410376230876)\n",
      "     | > stopnet_loss: 0.7324381470680237  (0.7303642511367797)\n",
      "     | > ga_loss: 0.007324855774641037  (0.010710274695108334)\n",
      "     | > decoder_diff_spec_loss: 0.12041228264570236  (0.11843464175860087)\n",
      "     | > postnet_diff_spec_loss: 4.166459560394287  (4.166129957834878)\n",
      "     | > decoder_ssim_loss: 0.9606598615646362  (0.9625176008542379)\n",
      "     | > postnet_ssim_loss: 0.9493577480316162  (0.9472919503847758)\n",
      "     | > loss: 5.782146453857422  (5.787148526509603)\n",
      "     | > align_error: 0.9792221207171679  (0.971007007136941)\n",
      "     | > grad_norm: tensor(1.9774, device='cuda:0')  (tensor(2.0732, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 0.4215  (0.343586057027181)\n",
      "     | > loader_time: 0.3301  (0.2629666614532471)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:36:24 -- STEP: 100/687 -- GLOBAL_STEP: 100\u001b[0m\n",
      "     | > decoder_loss: 6.077487468719482  (5.926449055671692)\n",
      "     | > postnet_loss: 8.078399658203125  (7.930570712089539)\n",
      "     | > stopnet_loss: 0.7315343022346497  (0.7309105211496355)\n",
      "     | > ga_loss: 0.0063105090521276  (0.0097247911291197)\n",
      "     | > decoder_diff_spec_loss: 0.11475232988595963  (0.11832864470779896)\n",
      "     | > postnet_diff_spec_loss: 4.1706461906433105  (4.165456624031067)\n",
      "     | > decoder_ssim_loss: 0.9622379541397095  (0.9623571407794952)\n",
      "     | > postnet_ssim_loss: 0.9466972351074219  (0.9472520756721496)\n",
      "     | > loss: 5.850642204284668  (5.792138104438784)\n",
      "     | > align_error: 0.982481662184  (0.9735641990602016)\n",
      "     | > grad_norm: tensor(1.9367, device='cuda:0')  (tensor(2.0374, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 0.5005  (0.3728156638145447)\n",
      "     | > loader_time: 0.4186  (0.2743708157539368)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:36:46 -- STEP: 125/687 -- GLOBAL_STEP: 125\u001b[0m\n",
      "     | > decoder_loss: 6.024819850921631  (5.955531204223633)\n",
      "     | > postnet_loss: 8.034607887268066  (7.959075550079346)\n",
      "     | > stopnet_loss: 0.732974648475647  (0.7312578129768372)\n",
      "     | > ga_loss: 0.005679345224052668  (0.00896063049510122)\n",
      "     | > decoder_diff_spec_loss: 0.11850506067276001  (0.11823426532745361)\n",
      "     | > postnet_diff_spec_loss: 4.161981582641602  (4.164407142639158)\n",
      "     | > decoder_ssim_loss: 0.9600991010665894  (0.9624051971435547)\n",
      "     | > postnet_ssim_loss: 0.9467990398406982  (0.947302734375)\n",
      "     | > loss: 5.823073863983154  (5.802800037384035)\n",
      "     | > align_error: 0.9842320997267962  (0.9754576589614153)\n",
      "     | > grad_norm: tensor(1.8783, device='cuda:0')  (tensor(2.0074, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 0.5526  (0.40279776763916014)\n",
      "     | > loader_time: 0.3681  (0.2887616195678712)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:37:11 -- STEP: 150/687 -- GLOBAL_STEP: 150\u001b[0m\n",
      "     | > decoder_loss: 6.258980751037598  (6.004268617630005)\n",
      "     | > postnet_loss: 8.262079238891602  (8.007453460693359)\n",
      "     | > stopnet_loss: 0.73293536901474  (0.7315689969062806)\n",
      "     | > ga_loss: 0.004989346954971552  (0.008324026959016923)\n",
      "     | > decoder_diff_spec_loss: 0.11866211146116257  (0.11788218448559443)\n",
      "     | > postnet_diff_spec_loss: 4.163304328918457  (4.16341798464457)\n",
      "     | > decoder_ssim_loss: 0.9629905819892883  (0.962441125313441)\n",
      "     | > postnet_ssim_loss: 0.9497544169425964  (0.9473517803351085)\n",
      "     | > loss: 5.936824321746826  (5.823892958958944)\n",
      "     | > align_error: 0.9863516595214605  (0.9771001959095399)\n",
      "     | > grad_norm: tensor(1.8819, device='cuda:0')  (tensor(1.9858, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 0.6278  (0.43680107275644936)\n",
      "     | > loader_time: 0.4062  (0.30271826585133876)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:37:37 -- STEP: 175/687 -- GLOBAL_STEP: 175\u001b[0m\n",
      "     | > decoder_loss: 6.383800029754639  (6.061662425994873)\n",
      "     | > postnet_loss: 8.385939598083496  (8.064732922145298)\n",
      "     | > stopnet_loss: 0.7329063415527344  (0.7317028079714094)\n",
      "     | > ga_loss: 0.004349741153419018  (0.007784568236342501)\n",
      "     | > decoder_diff_spec_loss: 0.11497090011835098  (0.11757630041667393)\n",
      "     | > postnet_diff_spec_loss: 4.153134346008301  (4.16281097957066)\n",
      "     | > decoder_ssim_loss: 0.9622629284858704  (0.9624843662125724)\n",
      "     | > postnet_ssim_loss: 0.9457457065582275  (0.947485351903098)\n",
      "     | > loss: 5.991118431091309  (5.849813782828194)\n",
      "     | > align_error: 0.9879946038126945  (0.9784755701465266)\n",
      "     | > grad_norm: tensor(1.8799, device='cuda:0')  (tensor(1.9679, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 0.6892  (0.46833725929260256)\n",
      "     | > loader_time: 0.6241  (0.3169368416922434)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:38:06 -- STEP: 200/687 -- GLOBAL_STEP: 200\u001b[0m\n",
      "     | > decoder_loss: 6.607649803161621  (6.12319048166275)\n",
      "     | > postnet_loss: 8.608768463134766  (8.126211566925047)\n",
      "     | > stopnet_loss: 0.7342799305915833  (0.7318700677156449)\n",
      "     | > ga_loss: 0.004181489814072847  (0.0073267458798363825)\n",
      "     | > decoder_diff_spec_loss: 0.11704990267753601  (0.11724176913499833)\n",
      "     | > postnet_diff_spec_loss: 4.151942729949951  (4.1619671082496605)\n",
      "     | > decoder_ssim_loss: 0.9647166728973389  (0.9625926610827445)\n",
      "     | > postnet_ssim_loss: 0.9493156671524048  (0.9476794594526291)\n",
      "     | > loss: 6.105048179626465  (5.8782245993614195)\n",
      "     | > align_error: 0.9882411397993565  (0.9796559318294749)\n",
      "     | > grad_norm: tensor(1.8595, device='cuda:0')  (tensor(1.9533, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 0.7266  (0.4984984564781189)\n",
      "     | > loader_time: 0.4137  (0.3297647440433502)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:38:36 -- STEP: 225/687 -- GLOBAL_STEP: 225\u001b[0m\n",
      "     | > decoder_loss: 6.59181022644043  (6.181057262420654)\n",
      "     | > postnet_loss: 8.595560073852539  (8.184033516777879)\n",
      "     | > stopnet_loss: 0.7339620590209961  (0.7320210138956705)\n",
      "     | > ga_loss: 0.0038219040725380182  (0.00693193593579862)\n",
      "     | > decoder_diff_spec_loss: 0.11484228819608688  (0.11690845681561364)\n",
      "     | > postnet_diff_spec_loss: 4.167508125305176  (4.161225420633949)\n",
      "     | > decoder_ssim_loss: 0.9602106213569641  (0.9625279397434658)\n",
      "     | > postnet_ssim_loss: 0.9472420811653137  (0.9477860617637635)\n",
      "     | > loss: 6.097364902496338  (5.905065388149685)\n",
      "     | > align_error: 0.9890675218775868  (0.9806727355966965)\n",
      "     | > grad_norm: tensor(1.8341, device='cuda:0')  (tensor(1.9414, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 0.8326  (0.5283181730906167)\n",
      "     | > loader_time: 0.4616  (0.3416528945498996)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:39:10 -- STEP: 250/687 -- GLOBAL_STEP: 250\u001b[0m\n",
      "     | > decoder_loss: 6.805828094482422  (6.248646039962768)\n",
      "     | > postnet_loss: 8.811065673828125  (8.251575187683097)\n",
      "     | > stopnet_loss: 0.7336784601211548  (0.7321056063175202)\n",
      "     | > ga_loss: 0.0033382682595402002  (0.006587067629210653)\n",
      "     | > decoder_diff_spec_loss: 0.11268095672130585  (0.1164520303606987)\n",
      "     | > postnet_diff_spec_loss: 4.151970863342285  (4.160348724365231)\n",
      "     | > decoder_ssim_loss: 0.9589244723320007  (0.9625126991271973)\n",
      "     | > postnet_ssim_loss: 0.9492978453636169  (0.9480052747726441)\n",
      "     | > loss: 6.197811126708984  (5.936925966262818)\n",
      "     | > align_error: 0.9900644272565842  (0.9815581897385418)\n",
      "     | > grad_norm: tensor(1.8473, device='cuda:0')  (tensor(1.9315, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 0.9003  (0.561660090446472)\n",
      "     | > loader_time: 0.4747  (0.35902712726593017)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:39:47 -- STEP: 275/687 -- GLOBAL_STEP: 275\u001b[0m\n",
      "     | > decoder_loss: 6.870790958404541  (6.307062856500799)\n",
      "     | > postnet_loss: 8.865826606750488  (8.30988063812255)\n",
      "     | > stopnet_loss: 0.731998085975647  (0.7321870576251638)\n",
      "     | > ga_loss: 0.003230385249480605  (0.006286382377994333)\n",
      "     | > decoder_diff_spec_loss: 0.1121039092540741  (0.11606877337802537)\n",
      "     | > postnet_diff_spec_loss: 4.144927024841309  (4.159497241973874)\n",
      "     | > decoder_ssim_loss: 0.964495837688446  (0.9625367760658264)\n",
      "     | > postnet_ssim_loss: 0.9506599307060242  (0.9481854382428256)\n",
      "     | > loss: 6.225351333618164  (5.96442694057118)\n",
      "     | > align_error: 0.9899555519223213  (0.9823274163292213)\n",
      "     | > grad_norm: tensor(1.8140, device='cuda:0')  (tensor(1.9224, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 0.9284  (0.5950932468067515)\n",
      "     | > loader_time: 0.5683  (0.3742809018221768)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:40:23 -- STEP: 300/687 -- GLOBAL_STEP: 300\u001b[0m\n",
      "     | > decoder_loss: 7.056303977966309  (6.3610639413197845)\n",
      "     | > postnet_loss: 9.062061309814453  (8.363775370915722)\n",
      "     | > stopnet_loss: 0.7338396310806274  (0.7322496424118679)\n",
      "     | > ga_loss: 0.0030462522991001606  (0.006024735510970157)\n",
      "     | > decoder_diff_spec_loss: 0.10972286015748978  (0.11571598979334034)\n",
      "     | > postnet_diff_spec_loss: 4.145753383636475  (4.1586518716812115)\n",
      "     | > decoder_ssim_loss: 0.9645953178405762  (0.9625059670209885)\n",
      "     | > postnet_ssim_loss: 0.9512732028961182  (0.9483686459064483)\n",
      "     | > loss: 6.321498394012451  (5.989893794059754)\n",
      "     | > align_error: 0.9905581548810005  (0.9829992248304188)\n",
      "     | > grad_norm: tensor(1.8303, device='cuda:0')  (tensor(1.9147, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 0.9062  (0.6239374740918477)\n",
      "     | > loader_time: 0.4911  (0.38632699886957805)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:41:01 -- STEP: 325/687 -- GLOBAL_STEP: 325\u001b[0m\n",
      "     | > decoder_loss: 7.006035327911377  (6.413298436678372)\n",
      "     | > postnet_loss: 9.01425552368164  (8.415833176832928)\n",
      "     | > stopnet_loss: 0.7311362624168396  (0.732308039848621)\n",
      "     | > ga_loss: 0.002997606061398983  (0.005794032357203275)\n",
      "     | > decoder_diff_spec_loss: 0.11241048574447632  (0.1153891425178601)\n",
      "     | > postnet_diff_spec_loss: 4.148238658905029  (4.157677646050086)\n",
      "     | > decoder_ssim_loss: 0.9599586725234985  (0.9625043361003582)\n",
      "     | > postnet_ssim_loss: 0.9491016268730164  (0.9485333792979901)\n",
      "     | > loss: 6.293624401092529  (6.014587258559006)\n",
      "     | > align_error: 0.9908006340265274  (0.9836054990899105)\n",
      "     | > grad_norm: tensor(1.8202, device='cuda:0')  (tensor(1.9080, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 0.9444  (0.650084773577177)\n",
      "     | > loader_time: 0.5322  (0.39680929770836454)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:41:40 -- STEP: 350/687 -- GLOBAL_STEP: 350\u001b[0m\n",
      "     | > decoder_loss: 7.159573078155518  (6.462205290113175)\n",
      "     | > postnet_loss: 9.16545581817627  (8.464541876656666)\n",
      "     | > stopnet_loss: 0.7329988479614258  (0.7323211022785732)\n",
      "     | > ga_loss: 0.002885238965973258  (0.00558833347234343)\n",
      "     | > decoder_diff_spec_loss: 0.10944194346666336  (0.11505403465458323)\n",
      "     | > postnet_diff_spec_loss: 4.147958278656006  (4.156812681470597)\n",
      "     | > decoder_ssim_loss: 0.9657481908798218  (0.9625631994860513)\n",
      "     | > postnet_ssim_loss: 0.9526768326759338  (0.9487121544565473)\n",
      "     | > loss: 6.372638702392578  (6.0377351011548726)\n",
      "     | > align_error: 0.991295924410224  (0.9841411353328399)\n",
      "     | > grad_norm: tensor(1.8285, device='cuda:0')  (tensor(1.9022, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 1.0177  (0.675869988713946)\n",
      "     | > loader_time: 0.5186  (0.4080144228254045)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:42:21 -- STEP: 375/687 -- GLOBAL_STEP: 375\u001b[0m\n",
      "     | > decoder_loss: 7.163954257965088  (6.508418655395507)\n",
      "     | > postnet_loss: 9.170872688293457  (8.510637201944988)\n",
      "     | > stopnet_loss: 0.7326856851577759  (0.732392571926117)\n",
      "     | > ga_loss: 0.0028029356617480516  (0.005404918729017178)\n",
      "     | > decoder_diff_spec_loss: 0.11163417994976044  (0.11475678952534991)\n",
      "     | > postnet_diff_spec_loss: 4.154165267944336  (4.155985029856364)\n",
      "     | > decoder_ssim_loss: 0.9605061411857605  (0.9625714612007141)\n",
      "     | > postnet_ssim_loss: 0.9504961967468262  (0.948892056465149)\n",
      "     | > loss: 6.374607563018799  (6.059732490539551)\n",
      "     | > align_error: 0.9913470223546028  (0.9846246444831291)\n",
      "     | > grad_norm: tensor(1.8143, device='cuda:0')  (tensor(1.8972, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 1.1281  (0.7021975860595705)\n",
      "     | > loader_time: 0.5503  (0.41910965220133456)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:43:04 -- STEP: 400/687 -- GLOBAL_STEP: 400\u001b[0m\n",
      "     | > decoder_loss: 7.240581512451172  (6.5509902274608605)\n",
      "     | > postnet_loss: 9.246881484985352  (8.553108150959016)\n",
      "     | > stopnet_loss: 0.7324368357658386  (0.7324044448137284)\n",
      "     | > ga_loss: 0.0027447640895843506  (0.005238093173247762)\n",
      "     | > decoder_diff_spec_loss: 0.10880425572395325  (0.11446414466947315)\n",
      "     | > postnet_diff_spec_loss: 4.144262313842773  (4.155201789140699)\n",
      "     | > decoder_ssim_loss: 0.9594491124153137  (0.9625588393211365)\n",
      "     | > postnet_ssim_loss: 0.9519469141960144  (0.949046173542738)\n",
      "     | > loss: 6.409142017364502  (6.0799372613430025)\n",
      "     | > align_error: 0.9913700427860022  (0.9850504760211334)\n",
      "     | > grad_norm: tensor(1.8014, device='cuda:0')  (tensor(1.8918, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 1.1107  (0.7277974170446397)\n",
      "     | > loader_time: 0.6233  (0.4309674900770187)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:43:47 -- STEP: 425/687 -- GLOBAL_STEP: 425\u001b[0m\n",
      "     | > decoder_loss: 7.171382904052734  (6.589811635858871)\n",
      "     | > postnet_loss: 9.177703857421875  (8.591820355583646)\n",
      "     | > stopnet_loss: 0.7330420017242432  (0.7324441644724677)\n",
      "     | > ga_loss: 0.002692043548449874  (0.005088382615324329)\n",
      "     | > decoder_diff_spec_loss: 0.10996601730585098  (0.11417771718081304)\n",
      "     | > postnet_diff_spec_loss: 4.146450519561768  (4.154512397541723)\n",
      "     | > decoder_ssim_loss: 0.9627327919006348  (0.9625400947122013)\n",
      "     | > postnet_ssim_loss: 0.9514669179916382  (0.9491798172277562)\n",
      "     | > loss: 6.37642765045166  (6.0983966030794035)\n",
      "     | > align_error: 0.9917331943288445  (0.9854467426481492)\n",
      "     | > grad_norm: tensor(1.8043, device='cuda:0')  (tensor(1.8873, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 1.0885  (0.751194985894596)\n",
      "     | > loader_time: 0.5588  (0.4408412950179155)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:44:31 -- STEP: 450/687 -- GLOBAL_STEP: 450\u001b[0m\n",
      "     | > decoder_loss: 7.326602935791016  (6.627597236633299)\n",
      "     | > postnet_loss: 9.327034950256348  (8.629456070794006)\n",
      "     | > stopnet_loss: 0.7330885529518127  (0.7324811842706468)\n",
      "     | > ga_loss: 0.002599720610305667  (0.004951598284145196)\n",
      "     | > decoder_diff_spec_loss: 0.10926878452301025  (0.11392570048570631)\n",
      "     | > postnet_diff_spec_loss: 4.140150547027588  (4.153732804192441)\n",
      "     | > decoder_ssim_loss: 0.96148282289505  (0.9625670452912648)\n",
      "     | > postnet_ssim_loss: 0.9507657289505005  (0.9493006836043464)\n",
      "     | > loss: 6.449913024902344  (6.11638407919142)\n",
      "     | > align_error: 0.9916819371283054  (0.9858022270310256)\n",
      "     | > grad_norm: tensor(1.8223, device='cuda:0')  (tensor(1.8832, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 1.1476  (0.7720667876137629)\n",
      "     | > loader_time: 0.6374  (0.44976159837510843)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:45:16 -- STEP: 475/687 -- GLOBAL_STEP: 475\u001b[0m\n",
      "     | > decoder_loss: 7.20700216293335  (6.661800658577366)\n",
      "     | > postnet_loss: 9.210419654846191  (8.663571670933782)\n",
      "     | > stopnet_loss: 0.7324758172035217  (0.7325123505843313)\n",
      "     | > ga_loss: 0.002548835938796401  (0.004827088122011015)\n",
      "     | > decoder_diff_spec_loss: 0.10901482403278351  (0.11368080594037706)\n",
      "     | > postnet_diff_spec_loss: 4.140448093414307  (4.153019026706097)\n",
      "     | > decoder_ssim_loss: 0.9632642865180969  (0.9625496825418974)\n",
      "     | > postnet_ssim_loss: 0.9519063830375671  (0.9494123038492704)\n",
      "     | > loss: 6.390733242034912  (6.132656347375167)\n",
      "     | > align_error: 0.9920453000813723  (0.9861298954506453)\n",
      "     | > grad_norm: tensor(1.7993, device='cuda:0')  (tensor(1.8793, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 1.3234  (0.793796003743222)\n",
      "     | > loader_time: 0.5996  (0.45918127160323285)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:46:02 -- STEP: 500/687 -- GLOBAL_STEP: 500\u001b[0m\n",
      "     | > decoder_loss: 7.2498016357421875  (6.696398607254028)\n",
      "     | > postnet_loss: 9.255081176757812  (8.698138483047494)\n",
      "     | > stopnet_loss: 0.7309625744819641  (0.7325203320980072)\n",
      "     | > ga_loss: 0.002492665546014905  (0.004712239646352827)\n",
      "     | > decoder_diff_spec_loss: 0.1110762283205986  (0.11340194399654864)\n",
      "     | > postnet_diff_spec_loss: 4.145693302154541  (4.1522641687393245)\n",
      "     | > decoder_ssim_loss: 0.9618484377861023  (0.9625109760761261)\n",
      "     | > postnet_ssim_loss: 0.9521690607070923  (0.9495248421430588)\n",
      "     | > loss: 6.412343502044678  (6.14914130115509)\n",
      "     | > align_error: 0.9921010481193662  (0.9864275623336435)\n",
      "     | > grad_norm: tensor(1.7982, device='cuda:0')  (tensor(1.8757, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 1.2577  (0.8146986870765687)\n",
      "     | > loader_time: 0.6241  (0.46784350109100337)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:46:52 -- STEP: 525/687 -- GLOBAL_STEP: 525\u001b[0m\n",
      "     | > decoder_loss: 7.342836380004883  (6.730230005355107)\n",
      "     | > postnet_loss: 9.341979026794434  (8.731833674112965)\n",
      "     | > stopnet_loss: 0.7323119640350342  (0.732538340205238)\n",
      "     | > ga_loss: 0.0024503397289663553  (0.004606125906908087)\n",
      "     | > decoder_diff_spec_loss: 0.1073509082198143  (0.11312199467704408)\n",
      "     | > postnet_diff_spec_loss: 4.138373851776123  (4.151458333333339)\n",
      "     | > decoder_ssim_loss: 0.9626545906066895  (0.9624655610039121)\n",
      "     | > postnet_ssim_loss: 0.9515267610549927  (0.9496222424507141)\n",
      "     | > loss: 6.45574426651001  (6.165251935323079)\n",
      "     | > align_error: 0.9920418243855238  (0.9867022231815471)\n",
      "     | > grad_norm: tensor(1.7820, device='cuda:0')  (tensor(1.8723, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 1.2481  (0.8379249790736608)\n",
      "     | > loader_time: 0.6269  (0.47866699627467557)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:47:42 -- STEP: 550/687 -- GLOBAL_STEP: 550\u001b[0m\n",
      "     | > decoder_loss: 7.522279739379883  (6.761888773658058)\n",
      "     | > postnet_loss: 9.52197265625  (8.763480860970247)\n",
      "     | > stopnet_loss: 0.7324317693710327  (0.7325442241538657)\n",
      "     | > ga_loss: 0.0024006429594010115  (0.004508062410591678)\n",
      "     | > decoder_diff_spec_loss: 0.1049327626824379  (0.11285514544356949)\n",
      "     | > postnet_diff_spec_loss: 4.125094890594482  (4.150681499134415)\n",
      "     | > decoder_ssim_loss: 0.9622710943222046  (0.9624493013728749)\n",
      "     | > postnet_ssim_loss: 0.9516828060150146  (0.9497334263541481)\n",
      "     | > loss: 6.541493892669678  (6.180356796438044)\n",
      "     | > align_error: 0.9927078746259212  (0.9869602822275324)\n",
      "     | > grad_norm: tensor(1.8134, device='cuda:0')  (tensor(1.8690, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 1.3105  (0.8599006791548296)\n",
      "     | > loader_time: 0.6506  (0.48673665696924384)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:48:34 -- STEP: 575/687 -- GLOBAL_STEP: 575\u001b[0m\n",
      "     | > decoder_loss: 7.472128868103027  (6.796835987256921)\n",
      "     | > postnet_loss: 9.475753784179688  (8.798390521173905)\n",
      "     | > stopnet_loss: 0.7343762516975403  (0.7325685813115994)\n",
      "     | > ga_loss: 0.0023576896637678146  (0.0044169785094487915)\n",
      "     | > decoder_diff_spec_loss: 0.10719045996665955  (0.11255870773740431)\n",
      "     | > postnet_diff_spec_loss: 4.129556655883789  (4.149856479478924)\n",
      "     | > decoder_ssim_loss: 0.9610536098480225  (0.9624020428242891)\n",
      "     | > postnet_ssim_loss: 0.9521132111549377  (0.9498568320274353)\n",
      "     | > loss: 6.5206146240234375  (6.1971286334162174)\n",
      "     | > align_error: 0.9924192135222256  (0.987201589576738)\n",
      "     | > grad_norm: tensor(1.7848, device='cuda:0')  (tensor(1.8664, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 1.3997  (0.882859984273496)\n",
      "     | > loader_time: 0.6582  (0.49493774206742)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:49:27 -- STEP: 600/687 -- GLOBAL_STEP: 600\u001b[0m\n",
      "     | > decoder_loss: 7.658829212188721  (6.831536526679995)\n",
      "     | > postnet_loss: 9.657059669494629  (8.833002193768834)\n",
      "     | > stopnet_loss: 0.7334592938423157  (0.7325930482149128)\n",
      "     | > ga_loss: 0.0023433503229171038  (0.00433213354825663)\n",
      "     | > decoder_diff_spec_loss: 0.10386445373296738  (0.11225960742682214)\n",
      "     | > postnet_diff_spec_loss: 4.126801013946533  (4.149060770670578)\n",
      "     | > decoder_ssim_loss: 0.9591897130012512  (0.9623368704319)\n",
      "     | > postnet_ssim_loss: 0.9539544582366943  (0.9499750209848086)\n",
      "     | > loss: 6.610101222991943  (6.213796482086183)\n",
      "     | > align_error: 0.9927590643055737  (0.987423487273821)\n",
      "     | > grad_norm: tensor(1.8180, device='cuda:0')  (tensor(1.8637, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 1.4926  (0.9053078885873159)\n",
      "     | > loader_time: 0.756  (0.5046793135007224)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:50:25 -- STEP: 625/687 -- GLOBAL_STEP: 625\u001b[0m\n",
      "     | > decoder_loss: 7.85305118560791  (6.867502915954591)\n",
      "     | > postnet_loss: 9.853677749633789  (8.868902830505384)\n",
      "     | > stopnet_loss: 0.7328912615776062  (0.7326108332633978)\n",
      "     | > ga_loss: 0.002314592245966196  (0.0042521851435303685)\n",
      "     | > decoder_diff_spec_loss: 0.10498251765966415  (0.11195695105791087)\n",
      "     | > postnet_diff_spec_loss: 4.131222248077393  (4.1482095413208055)\n",
      "     | > decoder_ssim_loss: 0.9599905610084534  (0.9622676491737365)\n",
      "     | > postnet_ssim_loss: 0.9530765414237976  (0.9500910927772522)\n",
      "     | > loss: 6.708464622497559  (6.2311045204162605)\n",
      "     | > align_error: 0.9935302548110485  (0.9876355430334807)\n",
      "     | > grad_norm: tensor(1.8276, device='cuda:0')  (tensor(1.8613, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 1.6804  (0.9305888164520264)\n",
      "     | > loader_time: 0.7043  (0.5152146736145021)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:51:59 -- STEP: 650/687 -- GLOBAL_STEP: 650\u001b[0m\n",
      "     | > decoder_loss: 7.762441635131836  (6.906691336264979)\n",
      "     | > postnet_loss: 9.758872985839844  (8.90793763527504)\n",
      "     | > stopnet_loss: 0.7345311045646667  (0.7326608582643368)\n",
      "     | > ga_loss: 0.0022555524483323097  (0.004176467444317845)\n",
      "     | > decoder_diff_spec_loss: 0.10613863170146942  (0.11162850636702314)\n",
      "     | > postnet_diff_spec_loss: 4.12422513961792  (4.14731253110446)\n",
      "     | > decoder_ssim_loss: 0.9613678455352783  (0.9621486507012293)\n",
      "     | > postnet_ssim_loss: 0.9528340697288513  (0.9502093155567463)\n",
      "     | > loss: 6.662278652191162  (6.250025205612184)\n",
      "     | > align_error: 0.9936379576101899  (0.9878491178391358)\n",
      "     | > grad_norm: tensor(1.7994, device='cuda:0')  (tensor(1.8596, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 4.6533  (1.0090880746107835)\n",
      "     | > loader_time: 0.758  (0.5249729457268351)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-10 00:57:19 -- STEP: 675/687 -- GLOBAL_STEP: 675\u001b[0m\n",
      "     | > decoder_loss: 7.7866716384887695  (6.944604374920882)\n",
      "     | > postnet_loss: 9.784215927124023  (8.945655958387594)\n",
      "     | > stopnet_loss: 0.7342635989189148  (0.7327317523956306)\n",
      "     | > ga_loss: 0.0013158139772713184  (0.00409237738991915)\n",
      "     | > decoder_diff_spec_loss: 0.10590770095586777  (0.11138096542270091)\n",
      "     | > postnet_diff_spec_loss: 4.127649307250977  (4.146470404730909)\n",
      "     | > decoder_ssim_loss: 0.9409787654876709  (0.9617857362605907)\n",
      "     | > postnet_ssim_loss: 0.9327978491783142  (0.9500378812683953)\n",
      "     | > loss: 6.660398006439209  (6.2681774881151)\n",
      "     | > align_error: 0.996161958668381  (0.9880970179420654)\n",
      "     | > grad_norm: tensor(1.8024, device='cuda:0')  (tensor(1.8583, device='cuda:0'))\n",
      "     | > current_lr: 2.5000000000000002e-08 \n",
      "     | > step_time: 60.4785  (1.4131040654359042)\n",
      "     | > loader_time: 1.1733  (0.5385671990006066)\n",
      "\n",
      " ! Run is removed from output/run-May-10-2024_12+28AM-0000000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/trainer/trainer.py\", line 1833, in fit\n",
      "    self._fit()\n",
      "  File \"/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/trainer/trainer.py\", line 1785, in _fit\n",
      "    self.train_epoch()\n",
      "  File \"/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/trainer/trainer.py\", line 1504, in train_epoch\n",
      "    outputs, _ = self.train_step(batch, batch_num_steps, cur_step, loader_start_time)\n",
      "  File \"/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/trainer/trainer.py\", line 1360, in train_step\n",
      "    outputs, loss_dict_new, step_time = self.optimize(\n",
      "  File \"/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/trainer/trainer.py\", line 1226, in optimize\n",
      "    outputs, loss_dict = self._compute_loss(\n",
      "  File \"/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/trainer/trainer.py\", line 1157, in _compute_loss\n",
      "    outputs, loss_dict = self._model_train_step(batch, model, criterion)\n",
      "  File \"/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/trainer/trainer.py\", line 1116, in _model_train_step\n",
      "    return model.train_step(*input_args)\n",
      "  File \"/tmp/ipykernel_1243/4217239134.py\", line 282, in train_step\n",
      "    outputs = self.forward(text_input, text_lengths, mel_input, mel_lengths, aux_input)\n",
      "  File \"/tmp/ipykernel_1243/4217239134.py\", line 166, in forward\n",
      "    postnet_outputs = self.postnet(decoder_outputs)\n",
      "  File \"/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/TTS/tts/layers/tacotron/tacotron2.py\", line 69, in forward\n",
      "    o = layer(o)\n",
      "  File \"/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/TTS/tts/layers/tacotron/tacotron2.py\", line 42, in forward\n",
      "    o = self.activation(o)\n",
      "  File \"/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/kitsu/miniconda3/envs/workEnv/lib/python3.10/site-packages/torch/nn/modules/activation.py\", line 356, in forward\n",
      "    return torch.tanh(input)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 25.26 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/trainer/trainer.py:1833\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1832\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1833\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/trainer/trainer.py:1785\u001b[0m, in \u001b[0;36mTrainer._fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_train_epoch \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_with_eval:\n\u001b[0;32m-> 1785\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrun_eval:\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/trainer/trainer.py:1504\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader):\n\u001b[0;32m-> 1504\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_num_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_start_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/trainer/trainer.py:1360\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self, batch, batch_n_steps, step, loader_start_time)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;66;03m# auto training with a single optimizer\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m     outputs, loss_dict_new, step_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_optimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m     loss_dict\u001b[38;5;241m.\u001b[39mupdate(loss_dict_new)\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/trainer/trainer.py:1226\u001b[0m, in \u001b[0;36mTrainer.optimize\u001b[0;34m(self, batch, model, optimizer, scaler, criterion, scheduler, config, optimizer_idx, step_optimizer, num_optimizers)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;66;03m# forward pass and loss computation\u001b[39;00m\n\u001b[0;32m-> 1226\u001b[0m outputs, loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_idx\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;66;03m# skip the rest if not outputs from the model\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/trainer/trainer.py:1157\u001b[0m, in \u001b[0;36mTrainer._compute_loss\u001b[0;34m(self, batch, model, criterion, config, optimizer_idx)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1157\u001b[0m         outputs, loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, loss_dict\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/trainer/trainer.py:1116\u001b[0m, in \u001b[0;36mTrainer._model_train_step\u001b[0;34m(batch, model, criterion, optimizer_idx)\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mtrain_step(\u001b[38;5;241m*\u001b[39minput_args)\n\u001b[0;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 282\u001b[0m, in \u001b[0;36mTacotron2.train_step\u001b[0;34m(self, batch, criterion)\u001b[0m\n\u001b[1;32m    281\u001b[0m aux_input \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeaker_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: speaker_ids, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_vectors\u001b[39m\u001b[38;5;124m\"\u001b[39m: d_vectors}\n\u001b[0;32m--> 282\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# set the [alignment] lengths wrt reduction factor for guided attention\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 166\u001b[0m, in \u001b[0;36mTacotron2.forward\u001b[0;34m(self, text, text_lengths, mel_specs, mel_lengths, aux_input)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# B x mel_dim x T_out\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m postnet_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m postnet_outputs \u001b[38;5;241m=\u001b[39m decoder_outputs \u001b[38;5;241m+\u001b[39m postnet_outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/TTS/tts/layers/tacotron/tacotron2.py:69\u001b[0m, in \u001b[0;36mPostnet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvolutions:\n\u001b[0;32m---> 69\u001b[0m     o \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m o\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/TTS/tts/layers/tacotron/tacotron2.py:42\u001b[0m, in \u001b[0;36mConvBNBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_normalization(o)\n\u001b[0;32m---> 42\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(o)\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/torch/nn/modules/activation.py:356\u001b[0m, in \u001b[0;36mTanh.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 25.26 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     TrainerArgs(), \n\u001b[1;32m      3\u001b[0m     config, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     eval_samples\u001b[38;5;241m=\u001b[39meval_samples\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/trainer/trainer.py:1862\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1861\u001b[0m traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m-> 1862\u001b[0m \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2121\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[1;32m   2119\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2120\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 2121\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2122\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/IPython/core/ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/IPython/core/ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 568\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    572\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/IPython/core/ultratb.py:1435\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[0;32m-> 1435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/IPython/core/ultratb.py:1326\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1323\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/IPython/core/ultratb.py:1173\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1166\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1170\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m   1171\u001b[0m ):\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/IPython/core/ultratb.py:1063\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m   1061\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[1;32m   1062\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1063\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m   1064\u001b[0m )\n\u001b[1;32m   1066\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1067\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/workEnv/lib/python3.10/site-packages/IPython/core/ultratb.py:1131\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[0;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m)\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    TrainerArgs(), \n",
    "    config, \n",
    "    \"/home/kitsu/course_work/TTS task\", \n",
    "    model=model, \n",
    "    train_samples=train_samples, \n",
    "    eval_samples=eval_samples\n",
    ")\n",
    "\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n",
      " > Text splitted to sentences.\n",
      "['Я Женя Ерофеев люблю сосать большие черные члены, а также чтобы меня насиловали день и ночь в общественном туалете']\n",
      " > Processing time: 1.9605977535247803\n",
      " > Real-time factor: 0.24261010856391646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output.wav'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
    "\n",
    "text = ''\n",
    "\n",
    "tts.tts_to_file(text=text, \n",
    "                speaker_wav='test_1.wav', \n",
    "                language=\"ru\", \n",
    "                file_path=\"output.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
